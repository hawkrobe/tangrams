{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.attrs import POS\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlp_utils import get_feats, lemmatize_doc, scramble_words\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import sparse2full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-process text by lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_to_use = 'tangramsSequential_collapsed'\n",
    "d_raw = pd.read_csv('../../data/{}.csv'.format(version_to_use))#.rename(index=str, columns={\"contents\": \"text\"})\n",
    "d_raw['text'] = [nlp(text) for text in d_raw['contents']]\n",
    "d_raw['lemmas'] = [lemmatize_doc(parsed_text) for parsed_text in d_raw['text']]\n",
    "docs_dict = Dictionary(d_raw['lemmas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go ahead and extract the 'content' words we'll use for extracting vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentful = [] \n",
    "for utterance in d_raw['text'] :\n",
    "    subset = []\n",
    "    for word in utterance :\n",
    "        if word.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN'] and word.has_vector:\n",
    "            subset.append(word)\n",
    "    contentful.append(subset)\n",
    "d_raw['contentful'] = contentful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we're missing rows so we need to 'fill in' the content so that it'll be NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d_raw.copy()\n",
    "d = d.set_index(['gameid','intendedName', 'repetitionNum'])\n",
    "mux = pd.MultiIndex.from_product([d.index.levels[0], d.index.levels[1],d.index.levels[2]], names=['gameid','intendedName', 'repetitionNum'])\n",
    "d = d.reindex(mux, fill_value=[np.nan]).reset_index()\n",
    "\n",
    "nan_rows = [i for (i,row) in d.iterrows() if pd.isna(row['text'])]\n",
    "nan_insert_rows = [k - lag for (lag, k) in enumerate(nan_rows)]\n",
    "\n",
    "gameidList = pd.unique(d.gameid.ravel()).tolist()\n",
    "tangramList = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create tf-idf weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_corpus = [docs_dict.doc2bow(doc) for doc in d['lemmas'] if not np.any(pd.isna(doc))]\n",
    "model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict)\n",
    "docs_tfidf  = model_tfidf[docs_corpus]\n",
    "docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])\n",
    "tfidf_emb_vecs = np.vstack([nlp(docs_dict[i]).vector for i in range(len(docs_dict))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_emb_raw = np.dot(docs_vecs, tfidf_emb_vecs) \n",
    "docs_emb = np.insert(docs_emb_raw, nan_insert_rows, np.nan, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine semantic embeddings\n",
    "We'd like to pull out bag of words embeddings from NPs in each utterance in the cued dataset and cluster them for each tangram; expect to see different pairs in different parts of the space (i.e. to compute a d' for an 'idiosyncracy' or 'multiple equilibria' result) and also different utterances from single games closer together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlp_utils import get_feats\n",
    "for i in range(100) :\n",
    "    print(i)\n",
    "    meta, raw_avg_feats, weighted_feats = get_feats(d, docs_emb, nlp, scramble=True)\n",
    "    np.save('../outputs/feats_tangrams_embeddings_rawavg_scrambled{}.npy'.format(i), raw_avg_feats)#, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, raw_avg_feats = get_feats(d, docs_emb, nlp, scramble=False)\n",
    "meta.to_csv('../outputs/meta_tangrams_embeddings.csv')\n",
    "np.save('../outputs/feats_tangrams_embeddings_rawavg.npy', raw_avg_feats)#, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at tsne visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "tsne = TSNE(n_components = 2, n_iter=2000)\n",
    "big_pca = PCA(n_components = 50)\n",
    "viz_pca = PCA(n_components = 2)\n",
    "mds = MDS(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d.query('\"angel\" in lemmas')\n",
    "angel_df = (d[d['contents'].str.contains(\"superman\",  na=False)]\n",
    "                  .query('intendedName == \"C\"'))\n",
    "angel_df['pos'] = [[r.pos_ for r in nlp(row)] for row in angel_df['contents']]\n",
    "print(angel_df)\n",
    "raw_avg_feats[angel_df.index]\n",
    "#meta.query('angel in contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_viz = pd.DataFrame(\n",
    "    columns = ['gameid', 'intendedName', 'repetitionNum', 'x_tsne', 'y_tsne']\n",
    ")\n",
    "\n",
    "for name, group in meta.groupby('intendedName') :\n",
    "    tangram_inds = np.array(group.index)\n",
    "    relevant_feats = raw_avg_feats[tangram_inds]\n",
    "    nan_rows = [i for i in range(relevant_feats.shape[0]) if pd.isna(relevant_feats[i,0])]\n",
    "    nan_insert_rows = [k - lag for (lag, k) in enumerate(nan_rows)]\n",
    "    X = np.ma.masked_invalid(relevant_feats)\n",
    "    tsne_out = tsne.fit_transform(big_pca.fit_transform(np.ma.compress_rows(X)))\n",
    "    tsne_out = np.insert(tsne_out, nan_insert_rows, np.nan, axis=0)\n",
    "    X_tsne = pd.DataFrame(tsne_out, \n",
    "                         columns = ['x_tsne', 'y_tsne'], \n",
    "                         index=tangram_inds) #X_mds, \n",
    "    embedding_viz = embedding_viz.append(pd.concat([group, X_tsne], axis = 1), \n",
    "                                         ignore_index=True, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_viz.to_csv('../outputs/embeddings.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
