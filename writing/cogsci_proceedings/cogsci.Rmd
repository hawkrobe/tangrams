---
title: "Convention-formation in iterated reference games"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: 
  \author{{\large \bf Robert X.D. Hawkins, Michael C. Frank, Noah D. Goodman} \\ \texttt{\{rxdh, mcfrank, ngoodman\}@stanford.edu} \\ Department of Psychology, Stanford University}
 
abstract: 
    "What cognitive mechanisms support the emergence of linguistic conventions from repeated interaction? We present results from a large-scale, multi-player replication of the classic *tangrams task*, focusing on three foundational properties of conventions: arbitrariness, stability, and reduction of utterance length over time. These results motivate a theory of convention-formation where agents, though initially uncertain about word meanings in context, assume others are using language with such knowledge. Thus, agents may learn about meanings by reasoning about a knowledgeable, informative partner; if all agents engage in such a process, they successfully coordinate their beliefs, giving rise to a conventional communication system. We formalize this theory in a computational model of language understanding as social inference and demonstrate that it produces all three properties in a simplified domain."
    
keywords:
    "conventions; pragmatics; communication"
    
output: cogsci2016::cogsci_paper
---


```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
library(devtools)
library(cogsci2016)
#library(langcog)
```

```{r, libraries, include = FALSE}
library(ggplot2)
library(lme4)
library(entropy)
library(tm)
library(tidyr)
library(dplyr)
library(stringr)
library(knitr)
library(readr)
#library(png)
library(grid)
library(ggplot2)
library(xtable)
library(rwebppl)
library(ggthemes)
# library(cowplot)
library(latex2exp)
```

# Introduction

Just as drivers depend on shared behavioral conventions to safely navigate traffic, successful communication depends on a set of shared linguistic conventions. Speakers of different languages around the world refer to the same object in many different ways, yet when ordering a coffee in San Francisco, one can confidently use the English word "coffee" and be understood. How do these conventions -- classically characterized by @Lewis69_Convention as arbitrary but stable solutions to recurring coordination problems -- form in the first place? 

While *global* conventions adopted and sustained throughout a large population of speakers may develop over longer time scales, we also effortlessly coordinate on *local* conventions -- or conceptual pacts [@BrennanClark96_ConceptualPactsConversation] -- within the span of a single dialogue. For example, when discussing possible conditions to use in an upcoming experiment, a team of collaborators might begin the meeting using long descriptions to refer to each condition but end the meeting using conventional terms like "condition A" and "condition B." Since global conventions are hypothesized to emerge through diffuse, repeated interactions of this more local kind [@GarrodDoherty94_GroupConventionsLinguistics], the cognitive mechanisms underlying convention-formation in such interactions are of foundational interest.


In a seminal study by @KraussWeinheimer64_ReferencePhrases, pairs of participants played a cooperative language game where they were presented with arrays of ambiguous shapes in randomized orders. The players were assigned the roles of *director* and *matcher* and allowed to talk freely. The matcher's goal was to rearrange their shapes to match the director's board, and the director's goal was to communicate useful descriptions. Over multiple rounds, descriptions were dramatically shortened: an early description like "upside-down martini glass in a wire stand," became simply "martini" by the end. Later studies [e.g. @ClarkWilkesGibbs86_ReferringCollaborative] refined this paradigm, using larger arrays of tangram-like figures and emphasizing the intricate back-and-forth process through which speakers and listeners negotiate over references. The referring expressions generated by participants in these studies revealed a number of rich qualitative phenomena. Here, we focus on three that are both prescribed top-down by theories of convention-formation and also arise bottom-up as major axes of variation in our data: 
arbitrariness, stability, and the systematic reduction of utterance length over time. 

*Arbitrariness* is a definitional property of conventions [@Lewis69_Convention]: there must be multiple solutions that would be equally successful as long as both players "agree" (e.g. driving on the left vs. right side of the road).
<!-- "cat" (English) and "chat" (French) are equally good ways of referring to a feline in their respective language communities. -->
<!-- In the context of language games, different pairs may converge on different but equally successful referring expressions.  -->
By the final round in a language game, for example, one pair might successfully use the expression 'dancer' to refer to a tangram, while another might use 'skater'.
The other definitional property we consider is *stability*: it is in everyone's best interest to keep using a convention once established. 
<!-- In our language games, this means that we expect players to stick with the same referring expressions over the course of the game, as opposed to switching to a novel description on the final round.  -->
Finally, *reduction* is more specific to the reference game paradigm and refers to the transformation of longer, complex expressions into simpler expressions over the course of interaction, as @KraussWeinheimer64_ReferencePhrases observed. While this broad phenomenon has been replicated many times, exactly what is reduced remains an open empirical question.

Theories of convention-formation differ primarily in the extent to which sophisticated social reasoning and common ground is required. At one extreme, agents use simple heuristic updating rules and do not need to represent or reason about other agents at all [@Young15_EvolutionOfSocialNorms;@CentolaBaronchelli15_ConventionEmergence;@Barr2004_ConventionalCommunicationSystems].
<!-- [@ShohamTennenholtz97_EmergenceOfConventions;@Delgado02_ConventionsNetworks;@Young15_EvolutionOfSocialNorms;@CentolaBaronchelli15_ConventionEmergence;@Barr2004_ConventionalCommunicationSystems].  -->
Simulations elegantly show how arbitrary signaling systems can spread and come to stably dominate large populations. However, due to their 'rich get richer' dynamic, it is not clear how simple heuristic updating mechanisms alone could account for reduction in repeated interaction. At the other extreme are theories in which agents recursively track what information is *mutual knowledge*, often formalized in a game theoretic setting [@Lewis69_Convention]. @WilkesGibbsClark92_CoordinatingBeliefs and others have proposed that agents engage in a collaborative process of actively establishing mutual knowledge, though the mechanisms allowing conventions to emerge under such conditions have not been instantiated in a formal model to our knowledge.

In this paper, we argue for a theoretical position on the spectrum between these poles: *conventions form when uncertain agents treat their partners' knowledge as ground truth*. In other words, agents assume their partner is knowledgeably and rationally using some conventional lexicon mapping labels to meanings but are themselves initially unsure of its identity. Through observing their partner's behavior in repeated actions, agents learn and adopt that lexicon, though their partner in fact begins in the same state of ignorance. When both agents independently adopt such a social learning strategy, they align to one another, coordinating on and implicitly creating shared conventions. 

To motivate this theory, we first conduct a large-scale replication of the tangrams task on the web, which has traditionally been limited to relatively small sample sizes in the lab. We use distributions of lexical and syntactic features in the text corpus to operationalize arbitrariness, stability, and reduction, which have been difficult to analyze at a fine-grained level due to the sparseness of existing data. Taking these insights into account, we then formalize our theory in a computational model of communication in repeated reference games based on recent successes capturing language understanding as social inference [@GoodmanStuhlmuller13_KnowledgeImplicature;@GoodmanFrank16_RSATiCS]. Finally, we show that this model qualitatively produces all three empirical signatures in a simplified domain inspired by the tangrams task.

```{r image, cache=T, fig.env = "figure", fig.pos = "tb", fig.align='center', fig.width=3, fig.height=3, num.cols.cap=1, fig.cap = "\\label{fig:taskScreenshot} Example trial in experimental interface. Both players could freely use the chat box, and the matcher could click and drag the tangram images."}
img <- png::readPNG("figs/directorBoard.png")
grid::grid.raster(img)
```

# Replication of the Tangrams task

To collect a corpus of reference game dialogue that supports more detailed analyses of convention-formation, we ported the tangrams task used in @ClarkWilkesGibbs86_ReferringCollaborative to a real-time, multi-player web environment. 

## Methods

```{r, cache=T}
# Import message data...
tangramMsgs = read_csv("../../data/analysis/tangrams/handTagged.csv") %>%
  rename(msgTime = time, 
         role = sender)

# Import survey data...
tangramSubjInfo = read.csv("../../data/tangrams_unconstrained/turk/tangrams-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  select(-workerid, -DirectorBoards, -initialMatcherBoards)

rawTangramCombined <- tangramMsgs %>% left_join(tangramSubjInfo, by = c('gameid', 'role'))

# Exclusion criteria
nonNativeSpeakerIDs <- unique((tangramSubjInfo %>% filter(nativeEnglish != "yes"))$gameid)
incompleteIDs <- unique((rawTangramCombined %>% group_by(gameid) %>% 
                           filter(length(unique(roundNum)) != 6))$gameid)
badGames <- union(incompleteIDs, nonNativeSpeakerIDs)

# filter & preprocess
tangramCombined <- rawTangramCombined %>%
  filter(!(gameid %in% badGames)) %>%
  mutate(numRawWords = str_count(contents, "\\S+")) %>%
  filter(!is.na(numRawWords)) # filter out pure punctuation messages

numGames <- length(unique(tangramCombined$gameid))
numUtterances <- length(tangramCombined$contents)
```

### Participants

200 participants were recruited from Amazon's Mechanical Turk and paired into dyads to play a real-time communication game using the framework in @Hawkins15_RealTimeWebExperiments. We excluded games that terminated before the completion of 6 rounds and where participants reported a native language different from English, leaving a corpus of `r numGames` complete games with a total of `r numUtterances` utterances.

### Stimuli

On every trial of the game, both participants were shown a $6 \times 2$ grid containing twelve tangram shapes, reproduced from @ClarkWilkesGibbs86_ReferringCollaborative. Cells were labeled with fixed numbers from one to twelve in order to help participants easily refer to locations in the grid (see Fig. \ref{fig:taskScreenshot}).

### Procedure

After passing a short quiz about task instructions, participants were randomly assigned the role of either 'director' or 'matcher' and automatically paired into virtual rooms containing a chat box and grid of stimuli. Both participants could freely use the chat box to communicate at any time. The director's tangrams were fixed in place, but the matcher could click and drag the shapes to reorder them. The director had to send messages about the locations of different tangrams on their fixed board; the matcher had to identify the corresponding tangram shapes and move them to the correct locations. When the players were satisfied that their boards matched, the matcher clicked a 'submit' button that gave players feedback on their score (out of 12) and scrambled the tangrams for the next round. After six rounds, players were redirected to a short exit survey. We collected the raw text of every message sent and every swapping action taken by the matcher^[Data are available at https://cocolab.stanford.edu/datasets/tangrams.html]. 

## Results 

```{r xtable, cache=T, results="asis"}
unigrams <- read_csv("../../analysis/tangrams/wordCounts.csv", col_names = T) %>%
  group_by(word, POS, roundNum) %>% 
  summarize(count = sum(count)) %>% 
  rowwise() %>%
  mutate(roundNum = paste0("round", roundNum, collapse = "")) %>%
  spread(roundNum, count) %>%
  filter(round1 > 10) %>%
  mutate(diffSize = round1 - round6,
         diffPct = (round1 - round6)/round1) %>% 
  arrange(desc(diffSize)) %>%
  select(word) %>% 
  filter(word != ',') %>%
  filter(word != '#') %>%
  head(n = 10)

bigrams <- read_csv("../../analysis/tangrams/bigramCounts.csv", col_names = T) %>%
  group_by(word, roundNum) %>% 
  summarize(count = sum(count)) %>% 
  rowwise() %>%
  mutate(roundNum = paste0("round", roundNum, collapse = "")) %>%
  spread(roundNum, count) %>%
  filter(round1 > 10) %>%
  mutate(diffSize = round1 - round6,
         diffPct = (round1 - round6)/round1) %>% 
  arrange(desc(diffSize)) %>% 
  select(word) %>% 
  rename(bigrams = word) %>%
  head(n = 10)

topWords <- t(cbind(unigrams, bigrams))
colnames(topWords) <- paste0(paste('#', seq(10), sep = ''))
rownames(topWords) <- c('unigrams', 'bigrams')
topWords.toPrint = xtable(topWords, label = 'tab:words', 
                          caption = 'Top 10 unigrams and bigrams with the highest reduction')
align(topWords.toPrint) <- paste0(c("|r||", rep('l|', 10)), collapse = "")
print(topWords.toPrint, floating.environment = "table*", comment=F, table.placement = 't')
```


```{r, eval=FALSE}
accuracy = read_csv('../../data/tangrams_unconstrained/finalBoard/tangramsFinalBoards.csv') %>%
  select(gameid, roundNum, score)

# Plot individual curves
ggplot(accuracy, aes(x = roundNum, y = score)) +
  geom_line() +
  facet_wrap(~ gameid)

# Plot means
accuracy %>%
  filter(score > 0) %>%
  group_by(roundNum) %>%
  multi_boot_standard("score") %>%
  ggplot(aes(x = roundNum, y = mean)) + 
    geom_point() +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper))

# Run mixed-effects regression
summary(lmer(score ~ roundNum + (1+roundNum| gameid), data = accuracy))
```

```{r replicationfig, cache = TRUE, fig.env = "figure*", fig.pos = "t!", fig.align='center', fig.width=7, fig.height=1.17, fig.show='hold', num.cols.cap=2, fig.cap='\\label{fig:replication} Reduction phenomena. From left: (1) mean message length in words per tangram, (2) mean number of listener messages, (3) proportion of utterances containing adjectival clauses, (4) proportion of utterances containing subordinate clauses. Error bars are bootstrapped 95\\% CIs.'}
posTags <- read_csv('../../analysis/tangrams/posTagged.csv')
  
lengthReduction = tangramCombined %>% 
   filter(role == "director") %>%
   group_by(gameid, roundNum) %>% 
   summarize(individualM = sum(numRawWords)/12) %>% 
   group_by(roundNum) %>% 
   multi_boot_standard("individualM") %>%
   mutate(measure = '# words per tangram') 

clauseReduction = posTags %>%
  group_by(roundNum) %>%
  mutate(clauses=as.logical(clauses)) %>%
  multi_boot_standard('clauses') %>%
  mutate(measure = '% subordinate clauses') 

aclReduction = posTags %>%
  group_by(roundNum) %>%
  mutate(acl=as.logical(acl)) %>%
  multi_boot_standard('acl') %>%
  mutate(measure = '% adjectival clauses') 

listenerMsgs <- tangramCombined %>%  
  group_by(gameid, roundNum, role) %>% 
  summarize(individualM = n()) %>% 
  ungroup() %>%
  complete(role, roundNum, gameid, fill = list(individualM = 0)) %>% 
  spread(role, individualM) 

listenerMsgs %>%    
   group_by(roundNum) %>% 
   multi_boot_standard("matcher") %>%
   mutate(measure = '# listener messages')  %>%
  rbind(lengthReduction) %>%
  rbind(clauseReduction) %>%
  rbind(aclReduction) %>%
  mutate(measure = factor(measure, levels = c(
    "# words per tangram", "# listener messages", 
    "% adjectival clauses","% subordinate clauses"))) %>%
  ggplot(aes(x = roundNum, y = mean)) +
    geom_line() +
    xlab('round #') +
    ylab('metric') +
    geom_errorbar(aes(ymax = ci_upper, ymin = ci_lower), width = .1) +
    facet_wrap(~ measure, nrow = 1, ncol = 4, scales = 'free') + 
    ylim(c(0, NA)) +
    theme_few(9)
```

```{r POSfig, cache = T, fig.env = "figure", fig.pos = "t!", fig.align='center', fig.width=3, fig.height=1.5, fig.show='hold', num.cols.cap=2, fig.cap='\\label{fig:pos} Reduction rates for different parts of speech. Error bars are bootstrapped 95\\% CIs.'}
posReduction = read.csv('../../analysis/tangrams/posTagged.csv', header =T) %>%
  # Count occurences of each POS on each round within games
  group_by(roundNum, gameid) %>% 
  summarize(numWords = sum(numWords),
            nouns = sum(nouns),
            #numbers = sum(numbers),
            verbs = sum(verbs),
            dets= sum(determiners),
            pronouns = sum(pronouns),
            preps = sum(prepositions),
            adjectives = sum(adjectives),
            adverbs = sum(adverbs)) %>%
  gather(POS, count, nouns:adverbs) %>%
  select(gameid, roundNum, POS, count) %>%
  # Need to put in ids to spread
  rowwise() %>% 
  mutate(id = row_number()) %>% 
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, count) %>%
  # Compute % reduction from first to last round
  mutate(diffPct = (round1 - round6)/round1) %>%
  group_by(POS) %>%
  # Filter out handful of people who skipped first round w/ negative %...
  filter(diffPct >= 0) %>% 
  filter(POS != "OTHER") %>%
  # Take mean & se over participants
  multi_boot_standard('diffPct') %>%
  mutate(cat = ifelse(POS %in% c('dets', 'pronouns', 'preps'), 'closed', 'open')) %>%
  # rearrange
  transform(POS=reorder(POS, -mean) )

detReductionRate <- (posReduction %>% filter(POS == 'dets'))$mean * 100
nounReductionRate <- (posReduction %>% filter(POS == 'nouns'))$mean * 100

ggplot(posReduction, aes(x = POS, y = mean, fill = cat)) +
  geom_bar(stat = 'identity') +
  geom_errorbar(aes(ymax = ci_upper, ymin = ci_lower), width = .1)+
  theme_few(9) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("% reduction") +
  xlab("Part of Speech category")
```

```{r}
# Note that this includes Nicki's old data...
tagCounts = tangramMsgs %>% group_by(tangramRef) %>% tally() %>% spread(tangramRef, n)
pctTagged = 100*(1-(tagCounts$None/sum(tagCounts)))
```

<!-- \textcolor{red}{rdh: as Mike points out, I think this was a valient but ultimately doomed attempt... We should either run the sequential version of the task, pay turkers to label the rest, or stick to analyses that don't require these tags (gotta leave something for the journal paper...)} -->

<!-- Since the task was conducted with minimally constrained language use, we began by tagging which tangram was being referred to, if any, in each message. Instead of hand-tagging nearly 10,000 items, we used a hybrid strategy. First, since many of the director's messages explicitly referred to a cell number, we first cross-referenced these numbers with the locations of each tangram in their array. This successfully labelled 38.4% of utterances. Next, we combined this automatically tagged data with a previously hand-tagged pilot corpus containing 3955 utterances, and created a 80/20 split to train and evaluate a classifier that maps utterances to tangram tags. Each utterance string was vectorized into a matrix of unigram and bigram counts, then transformed into a normalized tf-idf representation (which weights counts by their overall frequency in the corpus).  -->

<!-- We trained our logistic classifier using stochastic gradient descent, yielding 75% accuracy on the test split. We then constructed an ROC curve examining the tradeoff between the true positive and false positive rate for different confidence thresholds, and selected a cautious threshold of 90% confidence that minimized the false positive rate to <5%. Finally, we used this threshold to label the subset of our full corpus where we are sufficiently confident. We iterated this process with batches of hand-tagging until `r round(pctTagged,2)`% of the corpus was assigned a tag. -->

### Arbitrariness and stability

```{r entropy, cache = T}
getCounts <- function(contents) {
  corpus <- Corpus(VectorSource(paste(contents, collapse = " ")))
  return(colSums(as.matrix(DocumentTermMatrix(corpus))))
}

permutationTest <- function(df) {
  permuted = df %>% 
    group_by(roundNum) %>% 
    mutate(permutation = sample(contents)) %>%
    group_by(gameid) %>%
    summarize(ent = entropy(getCounts(permutation))) %>%
    ungroup() 
  return(mean(permuted$ent))
}

permutations <- replicate(1000, {
  permutationTest(tangramCombined)
})

# Note that we explicitly do *not* normalize entropies, as this
# removes the contribution of a bigger vocabulary (as we would expect)
trueEntropy <- mean((tangramCombined %>%
  group_by(gameid) %>%
  summarize(ent = entropy(getCounts(contents))) %>%
  ungroup())$ent)

# ggplot() + 
#   geom_histogram(aes(x = permutations), binwidth = .005) +
#   geom_vline(xintercept = trueEntropy, color = 'red') +
#   theme_few(9) +
#   xlab('entropy')
```

We begin by examining signatures of *arbitrariness* and *stability* in our data. We operationalize these concepts using the information-theoretic measure of entropy: 
$$H(W) = \sum_w P(w) \log P(w)$$
where $P(w)$ denotes the distribution over frequencies of word tokens used within a game. Broadly speaking, entropy measures the predictability of a distribution. It is maximized when all elements are equally likely and declines as the distribution becomes more structured, i.e. when the probability mass is concentrated on a small subset of elements.

To derive predictions, we consider a permutation-test null model in which utterances on each of the six rounds are scrambled across games, designed to break any existing structure in each game's idiosyncratic word distributions. The mean empirical entropy should only differ from the null distribution of entropies generated from this scrambling process if *both* arbitrariness and stability hold. 

First, note that if stability did *not* hold, scrambling would have no effect on the entropy within individual games: a given speaker would already use different words each round, and swapping out the identity of those words would not affect the entropy of the word distribution. There would be no structure to break. 

If stability holds but arbitrariness does not, all players would adopt the single optimal (non-arbitrary) way to refer to each tangram. Therefore, the entropy of their word distributions also should not be affected by scrambling: a speaker's real words would be swapped out for the same tokens generated by another speaker. Scrambling wouldn't break the structure of the distribution, because the structure would be the same for all participants. 

Finally, if both arbitrariness and stability hold, then different speakers would adopt different referring expressions that persist from round to round. Hence, scrambling should *increase* the average game's entropy from a relatively low level: each game's idiosyncratic, concentrated distribution of words would be mixed together to form more heterogeneous and therefore high-entropy distributions.

To test this prediction, we computed the average within-game entropy for 1000 different permutations of speaker utterances. Since this permutation scheme keeps the number of messages per participant constant and simply swaps out the content of those messages within each round, it controls for the fact that some speakers sent more messages than others and also that speakers in earlier rounds use more words (see next section). We found that our null distribution lay within the interval [`r round(min(permutations), 2)`, `r round(max(permutations), 2)`], which is significantly higher than the true entropy (averaged across games) of `r round(trueEntropy,2)`, $p < 0.001$. This pattern is consistent only with signatures of both arbitrariness and stability.

### Reduction

Next, we turn to a set of analyses examining reduction in utterance length over the course of the experiment. At the coarsest level, we find that the mean number of words used by speakers decreases over time (see Fig. \ref{fig:replication}). This decrease replicates a highly reliable reduction effect found throughout the literature on iterated reference games [@KraussWeinheimer64_ReferencePhrases;@BrennanClark96_ConceptualPactsConversation]. Likely due to our purely textual (vs. spoken) interface, participants in our task used significantly fewer words overall than previously reported (e.g. an average of 20 words on the 1st round, compared to 40 in @ClarkWilkesGibbs86_ReferringCollaborative)  The following analyses break down this broad reduction into a finer-grained set of phenomena. 

The next level of granularity motivating our model approach concerns which kinds of words are most likely to be dropped. Is the speaker adopting a shorthand where they drop uninformative function words, or are they simplifying or narrowing their descriptions by omitting meaningful details [@ClarkWilkesGibbs86_ReferringCollaborative]? We used the Stanford CoreNLP part-of-speech tagger [@Toutanova03_POStagging] to count the number of words belonging to each part of speech in each message. Fig. \ref{fig:pos} shows the percent reduction of different parts of speech from the first round to the sixth round. We find that determiners ('the', 'a', 'an') are the most likely class of words to be dropped with an `r round(detReductionRate)`% reduction rate, on average. Nouns ('dancer', 'rabbit') are the least likely class to be dropped with only an `r round(nounReductionRate)`% rate. Closed-class parts of speech are strictly more likely to be dropped than open-class parts of speech. 

While this finding is consistent with the possibility that speakers adopt a shorthand using more fragments as the game proceeds, we find a more complex dynamic by examining the table of unigrams and bigrams most likely to be dropped (see Table \ref{tab:words}). Note that alongside dropped articles ('a', 'the'), there are a number of words that form conjunctions ('and') and modifiers ('of', 'with', 'the right'). In other words, it may be more likely that when function words are dropped, it is primarily as part of larger grammatical units that provide additional information in identifying the target. 

We explicitly examined this hypothesis by running the Stanford constituency parser [@SchusterManning16], tagging the occurrence of 
subordinate/adverbial clauses ('sitting *facing left*') and 
adjectival clauses ('angel *that is praying*')
<!-- -->
^[Specifically, we used the Universal Dependencies tags \texttt{csubj, ccomp, xcomp}, 
<!-- -->
and \texttt{advcl} for subordinate clauses and \texttt{acl} for adjectival clauses [@SchusterManning16]]. 
We found that both were reduced over the course of the game (see Fig. \ref{fig:replication}), lending additional support for the hypothesis that whole meaningful clauses are increasingly omitted. 
This result prompts a characterization of reduction where, due to uncertainty at the outset about the usefulness of any particular lexical unit, initial phrases pile on multiple partially redundant modifiers and descriptors. 
As the game progresses and ambiguity of reference decreases, these additional meaningful units become less useful and can be dropped. 
We return to this characterization more formally within the scope of our model below. 

# Model

Here, we present a probabilistic model of language production under uncertainty, which captures several of the signature properties of conventions shown above. This model belongs to the family of Rational Speech Act (RSA) models, which have been successful in explaining a wide range of linguistic phenomena -- including scalar implicature, adjectival vagueness, overinformativeness, indirect questions, and non-literal language use -- as arising from a process of recursive social reasoning. Most previous applications of RSA have focused on the listener's problem of language comprehension, but the puzzle of conventionalization is primarily a question of speaker production. An $n$th order pragmatic speaker trying to convey a particular state of affairs $s \in \mathcal{S}$ assuming lexicon $\mathcal{L}$ is assumed to select an utterance $u \in \mathcal{U}$ by trading off its expected informativity (with respect to a rational listener agent) against its cost, usually based on length [@GoodmanFrank16_RSATiCS]:
<!-- -->
$$S_n(u | s, \mathcal{L}) \propto \exp{\left(\alpha \log L_{n-1}(s | u, \mathcal{L}) - \textrm{cost}(u)\right)}$$
<!-- -->
where $\alpha$ is a soft-max optimality parameter controlling the extent to which the speaker maximizes over listener informativity. The listener, in turn, inverts the speaker model to reason about what underlying state $s$ the speaker is trying to convey, given their utterance $u$:
<!-- -->
$$L_n(s | u, \mathcal{L}) \propto P(s) S_{n}(u | s, \mathcal{L})$$
<!-- -->
\indent This recursion bottoms out in a *literal listener* who directly looks up the meaning of the utterance in the lexicon:
<!-- -->
$$L_0(s | u, \mathcal{L}) \propto \mathcal{L}(u, s)\cdot P(s)$$
<!-- -->
\indent As in several other recent applications of RSA [@GrafEtAl16_BasicLevel], we use a graded semantics, where utterances are better or worse descriptions of particular referents. For instance, the utterance "dancer" may initially be expected to apply to a photorealistic image of a ballerina ($\mathcal{L}(\textrm{'dancer'}, \textit{ballerina}) = 0.99$) more than an abstract image of one ($\mathcal{L}(\textrm{'dancer'}, \textit{abstract ballerina}) =0.6$), but apply to both better than a non-category member like an image of a dog ($\mathcal{L}(\textrm{'dancer'}, \textit{dog}) = 0.05$).

Our approach to convention-formation begins with the additional assumption of *lexical uncertainty* [@SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS;@BergenLevyGoodman16_LexicalUncertainty]. In other words, we assume that instead of having perfect knowledge of $\mathcal{L}$, the listener has uncertainty over the exact meanings of lexical items in the current context (i.e. it is initially unclear which of the ambiguous tangram shapes "the dancer" might refer to). They begin with some prior $P(\mathcal{L})$ about the identity their partner's true lexicon, which may be initially biased toward certain meanings. By conditioning on repeated observations of their partner's behavior, they use Bayes rule to infer this true lexicon:
<!-- -->
$$P_{L_n}(\mathcal{L} | d) \propto P(\mathcal{L})\prod_i S_n(s_i|u_i, \mathcal{L})$$
<!-- -->
where $d = \{s_i, u_i\}$ is a set of observations of $s_i$ and $u_i$ coming from previous exchanges^[There is a broader debate over the timescales at which lexicons and lexicon learning mechanisms operate; here, we assume a discourse-level structure to the lexicon, where there is uncertainty over how words are used *in the given conversation*. See @FrankGoodmanTenenbaum09_Wurwur for a related approach at the scale of cross-situational word learning.]. The listener marginalizes over this posterior when interpreting the speaker's utterance:
<!-- -->
$$L_n(s | u, d) \propto \sum_\mathcal{L}P_{L_n}(\mathcal{L}|d)L_n(s|u,\mathcal{L})$$
<!-- -->
The speaker, in turn, considers what utterances would be most informative for such a listener:
<!-- -->
$$S_n(u | s, d) \propto \exp( \alpha\log\left(\sum_{\mathcal{L}} P_{S_n}(\mathcal{L} | d) L_{n-1}(s | u, \mathcal{L})\right) - \textrm{cost}(u) )$$
<!-- -->
where the posterior over lexica $P_{S_n}(\mathcal{L} | d)$, uses the listener likelihood $L_{n-1}$. For the purposes of this paper, we fix the depth of recursion at $n = 2$.
This model is implemented in the probabilistic programming language WebPPL [@GoodmanStuhlmuller14_DIPPL] ^[All results can be reproduced running our code in the browser at \url{http://forestdb.org/models/conventions.html}]. Following  @SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS, we begin by showing how a random initial choice is taken to be evidence for a particular lexicon and becomes the base for successful communication even though neither party knows its meaning at the outset.

## Results

### Arbitrariness and stability

```{r model1, cache=TRUE}
arbitrarinessModel <- function(alpha, beta) {
  res <- webppl(program_file = 'webpplModels/arbitrary.wppl',
               model_var = 'model',
               inference_opts = list(method="forward", samples=1000),
               data = list(alpha = alpha, beta = beta, numSteps = 6),
               data_var = 'params')
  accuracyRes = res %>%
    separate(Parameter, c('type', 'roundNum')) %>%
    mutate(alpha = as.character(alpha),
           beta = as.character(beta))
  return(accuracyRes)
}

arbitrarinessRes <- complete(data.frame(
    alpha=c(1,2,5),		
    beta =c(1)		
 ), alpha, beta) %>%		
   rowwise() %>%		
   do(arbitrarinessModel(.$alpha,.$beta))
```

```{r}
plot.accuracy <- arbitrarinessRes %>% 
  filter(type == 'acc') %>%
  group_by(roundNum, value, alpha, beta) %>%
  tally() %>%   
  group_by(roundNum, alpha, beta) %>%
  mutate(prob = n / sum(n)) %>%
  ungroup() %>%
  filter(value == TRUE) %>%
  mutate(roundNum = as.numeric(roundNum)) %>%
  ggplot(aes(x = roundNum, y = prob, color = alpha)) +
    ylab('accuracy') +
    xlab('round #') +
    geom_line() +
    geom_hline(aes(yintercept=.5), linetype = 2) +
    ylim(0,1) +
    theme_few(9) +
    theme( plot.margin=unit(x=c(0,0,0,0),units="mm"), 
           legend.spacing=unit(0,"cm"),
           legend.position="top")

mylabels = c(expression(paste(u[1], ',', s[1])),
             expression(paste(u[1], ',', s[2])))

plot.arbitrariness <- arbitrarinessRes %>%
  spread(type, value) %>%
  unite(intendedPair, intended, utt, sep = '<->', remove = F) %>% 
  unite(realPair, utt, response, sep = '<->', remove = F) %>% 
  group_by(Iteration, alpha) %>% 
  mutate(firstPair = first(realPair)) %>%
  mutate(firstSystem = ifelse(firstPair %in% c('label1<->t1', 'label2<->t2'),
                         'system1', 'system2')) %>%
  mutate(chosenSystem = ifelse(intendedPair %in% c('t1<->label1', 't2<->label2'),
                         'system1', 'system2')) %>%
  group_by(firstSystem, roundNum, chosenSystem, alpha) %>% 
  tally() %>%
  ungroup() %>%
  complete(firstSystem, roundNum, chosenSystem, alpha, fill = list(n = 0)) %>%  
  group_by(roundNum, chosenSystem, alpha) %>%
  mutate(prob = n/sum(n)) %>%
  ungroup() %>%
  filter(chosenSystem == 'system1') %>%
  ggplot(aes(x = roundNum, y = prob, group=interaction(alpha,firstSystem),linetype =firstSystem, color = alpha)) +
    ylab(TeX("$P(u_1, s_1)")) +
    xlab("round #") +
    geom_line() +
    geom_point() + 
    theme_few(9) +
    scale_linetype_discrete(name='initial', 
                            breaks=c('system1', 'system2'), 
                            labels = mylabels) +
    theme( plot.margin=unit(x=c(0,0,0,0),units="mm"), 
           legend.spacing=unit(0,"cm"),
           legend.position="top") +
    guides(color=FALSE)
```

```{r modelReduction, cache=TRUE}
conjunctionModel <- function(alpha, beta) {
  res <- webppl(program_file = 'webpplModels/conjunction.wppl', 
             model_var = 'model',
             inference_opts = list(method="forward", samples=1000),
             data = list(numSteps=6, alpha=alpha, beta=beta),
             data_var = 'params')
  return (res %>%
    select(-Chain) %>%
    filter(str_detect(Parameter, 'utt')) %>%
    separate(Parameter, c('type', 'roundNum')) %>%
    mutate(alpha = as.character(alpha),
           beta = as.character(beta)))
}

conjunctionRes <- complete(data.frame(
    alpha=c(14),		
    beta =c(.5)		
 ), alpha, beta) %>%		
   rowwise() %>%		
   do(conjunctionModel(.$alpha,.$beta))
```

```{r arbitrarinessFig, fig.env = "figure*", fig.pos = "t", fig.align='center', fig.width=7, fig.height=1.83, num.cols.cap=2, fig.cap = "\\label{fig:modelResults} (A) Probability of speaker using $u_1$ to refer to $s_1$, broken out by initial observation: while players are initially ambivalent between the two labels (arbitrariness), the initial mapping is likely to persist (stability). (B) Accuracy rises as speaker and listener align. (C) When conjunctions are introduced into the grammar, utterances get shorter over time (reduction)."}
library(tidyboot)
reductionRes <- conjunctionRes %>% 
  spread(type, value) %>%
  mutate(numWords = str_count(utt, "\\S+")) %>%
  group_by(roundNum) %>%
  tidyboot_mean(column = numWords)

plot.reduction <- ggplot(reductionRes, aes(x = roundNum, y = mean, group=1)) +
  ylab('mean # words') +
  xlab('round #') +
  geom_line() +
  geom_errorbar(aes(ymax = ci_upper, ymin = ci_lower), width = .1) +  
  theme_bw(9) +
  #ylim(1,1.4) +
  theme_few(9) +  
  theme(plot.margin=unit(x=c(1,0,0,0),units="mm"))
library(cowplot)
plot_grid(plot.arbitrariness, plot.accuracy, plot.reduction, labels = c('A', 'B', 'C'), ncol = 3, align = 'vh')
ggsave('../cogsci/figs/modelResults.pdf',  height = 6, width = 18,units = 'cm', useDingbats = F)
```

Consider an environment with two abstract shapes ($\{s_1, s_2\}$), where the speaker must choose between two utterances ($\{u_1, u_2\}$) incurring equal cost. Their prior $P(\mathcal{L})$ over the meaning of each utterance is given by a Beta distribution^[In our implementation, we enumerate over coarse-grained bins; preliminary experiments using variational inference on the full continuous distribution give similar results.], so on the first round both utterances are equally likely to apply to either shape. If the speaker was trying to get their partner to pick $s_1$, then, since each utterance is equally (un)informative, they would randomly sample one (say, $u_1$), and observe the listener's selection of a shape (say, $s_1$). On the next round, the speaker uses the observed pair $\{u_1, s_1\}$ to update their beliefs about their partner's true lexicon, uses these beliefs to generate a new utterance, and so on. To examine expected dynamics over multiple rounds, we forward sample many possible trajectories.

We observe several important qualitative effects in our simulations. First, the fact that a knowledgeable listener responds to utterance $u$ with $s$ provides evidence for lexicons in which $u$ is a good fit for $s$, hence the likelihood of the speaker using $u$ to refer to $s$ increases on subsequent rounds (see Fig.\ref{fig:modelResults}A). In other words, the initial symmetry between the meanings can be broken by initial random choices, leading to completely *arbitrary but stable mappings* in future rounds. Second, because the listener is also learning the lexicon from these observations under the same set of assumptions, they converge on a shared set of meanings; hence, expected *accuracy* rises on future rounds (see Fig. \ref{fig:modelResults}B). Third, because one's partner is assumed to be pragmatic, agents can also learn about *unheard* utterances. Observing $d = \{u_1, s_1\}$ also provides evidence that $u_2$ is *not* a good fit for $s_1$ by Gricean maxims: if $u_2$ were a better fit for $s_1$, the speaker would have used it instead [@Grice75_LogicConversation]. Finally, *failed references* lead to conventions just as effectively as successful references: if the speaker intends $s_1$ and says $u_1$, but then the listener incorrectly picks $s_2$, the speaker will take this as evidence that $u_1$ actually means $s_2$ in their partner's lexicon and become increasingly likely to use it that way on subsequent rounds.

### Reduction in utterance length

Finally, we show how our model explains reduction of utterance length over multiple interactions. For utterances to be reduced, of course, they must vary in length. Motivated by our empirical observation that meaningful clauses are the primary unit of reduction, we extend our grammar to include *conjunctions*. This is one of the simplest ways to constructing longer utterances compositionally from lexical primitives, using the product rule:
$$\mathcal{L}(u_i \textrm{ and } u_j, o) = \mathcal{L}(u_i, o) \times \mathcal{L}(u_j, o)$$
\indent Analogous to our tangram stimuli, which have many ambiguous features and figurative perspectives that may be evoked in speaker descriptions, we consider a simplified scenario where speakers can refer to two different features of the two objects $\{o_1, o_2\}$. The speaker has four primitive words at their disposal -- two words for shape ($\{u_{s1}, u_{s2}\}$) and two for color $\{u_{c1}, u_{c2}\}$ -- and has uncertainty over the initial meanings of all four.

While we established in the previous section that conventions can emerge over a reference game in the complete absence of initial preferences, players often bring such preferences to the table. A player who hears 'ice skater' on the first round of our tangrams task is more likely to select some objects more than others, even though they still have some uncertainty over its meaning in the context. To show that our model can accommodate this fact, we allow the speaker's initial prior meanings to be slightly biased. $u_{s1}$ and $u_{c1}$ are more likely to mean $o_1$; $u_{s2}$ and $u_{c2}$ are more likely to mean $o_2$.

We ran 1000 forward samples of 6 rounds of speaker-listener interaction, and averaged over the utterance length at each round ^[In our simulations, we used $\alpha = 10$ and found the basic reduction effect over a range of different biases]. Our results are shown in Figure \ref{fig:modelResults}C: the expected utterance length decreases systematically over each round. To illustrate in more detail how this dynamic is driven by an initial rational preference for redundancy relaxing as reference becomes more reliable, we walk step-by-step through a single trajectory. 

Consider a speaker who wants to refer to object $o_1$. They believe their knowledgeable partner is slightly more likely to interpret their language using a lexicon in which $u_{s1}$ and $u_{c1}$ apply to this object, due to their initial bias. However, there is still a reasonable chance that one or the other alone actually refers strongly to $o_2$ in the true lexicon. Thus, it is useful to produce the conjunction "$u_{s1}$ and $u_{c1}$" to hedge against this possibility, despite its higher cost. Upon observing the listener's response (say, $o_1$), the evidence is indeterminate about the separate meanings of $u_{s1}$ and $u_{c1}$ but both become increasingly likely to refer to $o_1$. In the trade-off between informativity and cost, the shorter utterances remain probable options. Once the speaker chooses one of them, the symmetry collapses and that utterance remains most probable in future rounds. In this way, meaningful sub-phrases are omitted over time as the speaker becomes more confident about the true lexicon. 


# General Discussion

In this paper, we revisited the classic phenomenon of convention-formation in a large-scale, text-based replication of the tangrams task.
We argued that several key qualitative patterns in the data -- arbitrariness, stability, and the reduction of utterance length over repeated interactions -- can be explained by our model of informative communication under lexical uncertainty. 
This model formalizes a theory where conventions emerge via uncertain agents who assume their partner is knowledgably and informatively using some conventional lexicon. Through repeated observations of their partner's actions, agents learn this lexicon, thereby coordinating and aligning to one another.

Theories of convention-formation vary in the extent to which social reasoning about common ground is required. 
Our agents lie on a spectrum between the heuristic updating agents of @Barr2004_ConventionalCommunicationSystems and the sophisticated agents of @ClarkWilkesGibbs86_ReferringCollaborative, who collaboratively build up explicit representations of mutual knowledge. 
Speakers and listeners in our model implicitly coordinate their beliefs through a shared history of observations, which serves as "common ground" in an informal sense. They make critical use of pragmatic, social reasoning in order to learn
meanings, but do not explicitly consider the fact that this history is shared, or represent their partner's own uncertainty. 

<!-- Recent studies have also called into question the extent to which linguistic conventions are, in fact, perfectly arbitrary [@DingemanseEtAl15_IconicityLanguage; @LewisFrank16_LengthOfWordsComplexity]. This forces us to consider exactly in what sense arbitrariness is an essential property of conventions. In our model, we moved to a more probabilistic notion of arbitrariness that does not require initial beliefs to be completely uniform, as in the game-theoretic cases considered by @Lewis69_Convention. Because speakers assume their partner is knowledgeable and choose utterances using a soft-max decision rule, choices that are relatively unlikely *a priori* -- say, calling a tangram an idiosyncratic name like "crazy chicken" or "touchdown Jesus" may nonetheless be sampled and subsequently used by both partners to form stable beliefs about the lexicon. When observing a large number of games, as we did in our experimental analyses, we can therefore observe substantial variability even though some descriptions may initially fit better than others.  -->

<!-- relatively little modeling work has focused on the cognitive mechanisms supporting emergence of local conventions during shorter dyadic interactions. In fact, the convergence on semi-arbitrary yet efficient referring expressions within an interaction poses some problems for these classes of models. Pure evolutionary models do not provide a mechanism for agents to adapt their signaling strategy within a lifetime, let alone within an extended interaction. Emergence-through-use models, such as the one proposed by @Barr2004_ConventionalCommunicationSystems, use simple updating rules based on success or failure in an interaction, but are not easily extended to richer language models and cannot straightforwardly explain the sharp reduction in word length across a repeated interaction with a single partner \textcolor{red}{rdh: might want to actually show this if I'm going to make that argument...} -->

By capturing reduction, which purely heuristic theories have not yet demonstrated, we showed that minimal assumptions of social reasoning go a long way in accounting for key phenomena. Still, our model falls short in some ways. For instance, because we do not provide a mechanisms for the listener agent to respond with confirmation, repair, or follow-up questions, we cannot make explicit predictions about the reduction in *listener messages* (as shown in Fig. \ref{fig:replication}) or the effect of listener input on the conventionalization process. These phenomena require our model to deal with planning over extended dialogues, and more sophisticated speech acts. Similarly, while our model was explicitly designed with linguistic conventions in mind, it remains to be seen whether the same formulation generalizes to broader behavioral conventions. For example, the real-time coordination games used in @HawkinsGoldstone16_SocialConventions may not require players to reason about a structured lexicon with noise, but an action policy representation may play a similar role. While there remain many complex aspects of convention-formation in communication games left for future research, our approach nonetheless serves as a lower bound on the degree of social reasoning needed to capture lexical conventions in these games.




# Acknowledgements

\small We thank Nicole Maslan for her contributions during piloting. This work was supported by ONR grant N00014-13-1-0788 and a Sloan Research Fellowship to NDG. RXDH was supported by the Stanford Graduate Fellowship and the National Science Foundation Graduate Research Fellowship under Grant No. DGE-114747.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
