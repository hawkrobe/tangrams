%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A template for Wiley article submissions.
% Developed by Overleaf. 
%
% Please note that whilst this template provides a 
% preview of the typeset manuscript for submission, it 
% will not necessarily be the final publication layout.
%
% Usage notes:
% The "blind" option will make anonymous all author, affiliation, correspondence and funding information.
% Use "num-refs" option for numerical citation and references style.
% Use "alpha-refs" option for author-year citation and references style.

\documentclass[alpha-refs]{wiley-article}
% \documentclass[blind,num-refs]{wiley-article}

% Add additional packages here if required
\usepackage{siunitx}
\usepackage{todonotes}
\graphicspath{{./figs/}}

% Update article type if known
\papertype{Original Article}
% Include section in journal if known, otherwise delete
%\paperfield{Journal Section}

\title{The dynamics of structure and content in repeated reference games}

% List abbreviations here, if any. Please note that it is preferred that abbreviations be defined at the first instance they appear in the text, rather than creating an abbreviations list.
%\abbrevs{ABC, a black cat; DEF, doesn't ever fret; GHI, goes home immediately.}

% Include full author names and degrees, when required by the journal.
% Use the \authfn to add symbols for additional footnotes and present addresses, if any. Usually start with 1 for notes about author contributions; then continuing with 2 etc if any author has a different present address.
\author[1]{Robert Hawkins}
\author[1]{Michael Frank}
\author[1,2]{Noah Goodman}

%\contrib[\authfn{1}]{Equally contributing authors.}

% Include full affiliation details for all authors
\affil[1]{Department of Psychology, Stanford University}
\affil[2]{Department of Computer Science, Stanford University}

\corraddress{Robert Hawkins, Author One PhD, Department, Institution, City, State or Province, Postal Code, Country}
\corremail{rxdh@stanford.edu}

% \presentadd[\authfn{2}]{Department, Institution, City, State or Province, Postal Code, Country}

\fundinginfo{Funder One, Funder One Department, Grant/Award Number: 123456, 123457 and 123458; Funder Two, Funder Two Department, Grant/Award Number: 123459}

% Include the name of the author that should appear in the running header
\runningauthor{Hawkins et al.}

\begin{document}

\maketitle

% ABSTRACT THOUGHTS
% Framework for providing a quantitative characterization of phenomena; tracking meaningful content changing over time. how is the structure different? semantics and syntax... contents and structure.

% 1. syntax changes systematically in same-ish way for *everybody* (i.e. pruning, nominalization)
% 2. a lot of semantic idiosyncracy but internally consistent (i.e. meaning-preserving) within games (i.e. initialize in different places and become less redundant). cleaving utterances by preserving the relevant things to make the distinctions that matter for this task. stable on meaning but decreasing redundancy... 

% continuity in semantics & lose overlappingness... 
% compare semantic embeddings within game vs. across game?
% compare syntax within game vs. across game

% similarity on words that are there at the end vs. not there at the end...
% qualitative -- pull out unigrams and bigrams that are maintained per tangram
% Hinge from big picture to "this is a case study or task" surprisingly answerable by new tools

\begin{abstract}
When talking with novel partners about novel referents, speakers must continually adapt to coordinate on meaning. 
How do speakers understand one another in conversation? 
% for increasingly effective, efficient communication.
% \emph{How} the structure and content of natural language referring expressions change through continued interaction. %While such adaptation has been demonstrated under a variety of conditions, p
% While such adaptation has been demonstrated under a variety of conditions, p
Here we draw upon recent advances in natural language processing to provide a quantitative characterization of the dynamics of such adaptation.
We collected a corpus ($>$15,000 utterances) of extended dyadic interactions in a classic repeated reference game task where pairs of partners had to coordinate on how to talk about initially difficult-to-describe \emph{tangram} stimuli.
While different pairs coordinate on a wide range of idiosyncratic solutions to the problem of reference, they do so in a highly path-dependent manner: words that are more discriminative in the initial context (i.e. that were used for one target more than others) are more likely to persist through the final round.
Structurally, we found that utterances reduce across pairs following similar trajectories: entire modifying syntactic units drop out as conventions are established, leaving only contentful open-class parts of speech.
These findings provide higher resolution into the quantitative dynamics of \emph{ad hoc} convention formation: based on a shared history of usage, words systematically acquire new meaning with a partner to support more efficient communication.

% Please include a maximum of seven keywords
\keywords{social coordination, conventions, semantics, syntax, natural language processing, semantic embeddings}
\end{abstract}

\section{Introduction}\label{introduction}

%% Statement of the computational problem that motivates interest in this phenomenon?
Communication poses a challenging coordination problem for social agents. 
In order to successfully understand one another, two speakers must share roughly the same underlying meanings of words \citep{Lewis69_Convention}.
While traditional information-theoretic accounts of communication typically assume these meanings are acquired in childhood and more or less fixed for adult speakers \cite{XXX}, these accounts face several challenges. 
%a crucial backbone is provided by , yet we frequently need to go beyond.
First, no two speakers share exactly the same lexicon \citep{davidson_nice_1986, clark_communal_1998}. 
Lingo, proper nouns, nicknames, slang, inside jokes, metaphors, and other creative uses of neologism all operate in a regime of uncertainty where the same word or phrase may \emph{a priori} mean something different (or nothing at all) to different partners in different contexts \cite{XXX}
Second, because we live in a changing environment, we constantly experience novel entities, events, thoughts, and feelings we want to talk about---complex referents for which we have no pre-existing conventions and real uncertainty about whether a novel compositional expression will mean to our partner exactly what we intend it to mean.
What do we do when our background conventions are sufficient --- when we have to talk about something we've never had to talk about before with a partner we've never met?

In this paper, we explore the possibility that agents solve this problem by coordinating on new \emph{ad hoc} conventions on the fly.
We describe an \emph{inferential} account of this rapid coordination, which builds on the collaborative account proposed by Clark \cite{}.
Testing the predictions of this account have become newly tractable using modern techniques from natural language processing (NLP). 

\emph{Repeated reference games}, first studies by \cite{KraussWeinheimer64_ReferencePhrases}, provide a natural and productive paradigm for studying rapid coordination on meaning.
Pairs of participants played a cooperative language game where they were presented with arrays of ambiguous shapes in randomized orders. 
The players were allowed to talk freely; their goal was to find the correspondence between their boards by producing mutually understood descriptions of the shapes. 
Critically, as particular shapes reappeared on successive rounds of the game, descriptions were dramatically shortened: a lengthy early description like ``the upside-down martini glass in a wire stand'' became simply ``martini'' by the end. 
In other words, these results suggested that speakers addressed the computational challenge of referring to novel objects with a novel partner by initially drawing on many sources of prior information from existing conventions and then \emph{adapting} with their partner based on shared history.

This signature reduction effect has been replicated under many conditions testing the boundaries of adaptation, manipulating the kinds of objects used as targets, the contexts in which the objects appear, the identity of one's partner across repetitions, the feedback available, and the medium participants use to communicate. (cite, cite, cite). 
While this canon of qualitative effects has played an influential role in shaping theories of social coordination in communication, particularly in debates over the role of common ground representations, a \emph{quantitative} characterization of precisely what gets reduced, and how, has remained elusive. 
Yet these details matter for a number of open questions. 
How systematic is the structure of changes over time?
To what extent are idiosyncratic\dots
\todo[inline]{Refine these questions}

Here we present a large corpus of messages sent in a web-based reference game where a speaker repeatedly refers to the same objects with the same partner. 

\todo[inline]{TODO: Motivate focus on questions of structure and content; set up alternative hypotheses}

\section{Methods}

\begin{figure}
\includegraphics[scale=.65]{designAndExample.pdf}
\caption{(A) Screenshots of the `free matching' and `cued' variants of the tangrams task; (B) Visualization of words used to refer to one tangram on the first round compared to the final round. \todo[inline]{Fix text/formatting in screenshots (maybe use schematic instead); think about less corny representation than wordcloud? figure out where to put the wavy guy, maybe add speech bubble to indicate words came from speaker?}}
\label{fig:design}
\end{figure}

\subsection{Repeated reference experiment}

To collect a large corpus of natural dialogue that supports computational analyses of how pairs coordinate on meaning over time, we faced two primary decisions.
First, to observe the \emph{dynamics} of conventions over time, we needed participants to repeatedly face a natural language task where  for performance.
Second, to observe the formative period of linguistic conventions, we required novel, ambiguous stimuli for which participants didn't already have strong initial conventions.
These challenge were satisfied by a \emph{repeated reference game} design in which participants refer to the same objects across multiple rounds as they build up a shared history of interaction, or ‘common ground,’ with their partner. 

We developed two variants of the game. 
A relatively unconstrained \emph{free-matching} version, and a more tightly controlled \emph{cued} version that allows for higher resolution analyses of how references to individual tangrams changed over time (see Fig. \ref{fig:design}). 
The \emph{free-matching} version was an exploratory sample, but we pre-registered our full pre-processing and analysis pipeline for the \emph{cued} version\footnote{osf.io/XXXXX}, so while we report results for both versions throughout, we privilege the \emph{cued} version as our confirmatory sample.

\subsubsection{Participants}\label{participants}

A total of XXX participants (YYY in the \emph{free-matching} version and ZZZ in the \emph{cued} version) were recruited from Amazon's Mechanical Turk and paired into dyads to play a real-time communication game using the framework in \cite{Hawkins15_RealTimeWebExperiments}. 

\subsection{Exclusion criteria}

 e implemented an exclusion criterion based on accuracy. 
% Because the classic work using the repeated reference game paradigm reported near ceiling accuracy for all pairs, and b
Specifically, we implemented a 66/66 rule, excluding pairs that got got fewer than 66\% tangrams correct (8 of 12) on fewer than 66\% of blocks (4 of 6). 
While the majority of pairs (X\%) were at ceiling accuracy by the final round, this excluded K pairs who on examination of their text appeared to be guessing or rushing to completion.
We also excluded games that terminated before the completion of the experiment, and where participants reported a native language different from English, leaving a corpus of X complete games with a total of X utterances.

\subsubsection{Stimuli \& Procedure}\label{stimuli}

Throughout the task, participants were shown a \(6 \times 2\) grid containing twelve tangram shapes, reproduced from \cite{clarkReferringCollaborativeProcess1986}.  
After passing a short quiz about task instructions, participants were randomly assigned the role of either `director' or `matcher' and automatically paired into virtual rooms containing a chat box and grid of stimuli. 
Both participants could freely use the chat box to communicate at any time. 

In the \emph{free-matching} version, our procedure closely followed \cite{clarkReferringCollaborativeProcess1986}. 
The director and matcher began each round with scrambled boards. 
The director's tangrams were fixed in place, but the matcher's could be clicked and dragged into new positions.
The players was instructed to communicate through the chat box such that the matcher could  rearrange their shapes to match the order of the director's board.
When the players were satisfied that their boards matched, the matcher clicked a `submit' button that gave players batched feedback on their score (out of 12) and scrambled the tangrams for the next round. 
After six rounds, players were redirected to a short exit survey. 
Cells were labeled with fixed numbers from one to twelve in order to help participants easily refer to locations in the grid (see Fig. \ref{fig:design}).

While this replicated design allows for highly naturalistic interaction, it poses several problems for text-based analyses. First, utterances must contain not only descriptions of the tangrams but also information about the intended location (e.g. '\emph{number 10} is the \dots'). Additionally, because there are no constraints on the sequence, participants can revisit tangrams out of order or mention multiple tangrams in a single message, making it difficult to isolate exactly which utterances referred to which tangrams without extensive hand-annotation. Finally, the design of the `submit' button makes it easy for players to occasionally advance to the next round without referring to all 12 tangrams. 

For the \emph{cued} version, then, we designed a straightforwardly sequential variation on the task where speakers are privately cued to refer to targets one-by-one and feedback is given on each round; this allows us to straightforwardly conduct analyses at the tangram-by-tangram level. On each trial, one of the twelve tangrams was privately highlighted for the director with a box as the \emph{target} (see Fig. \ref{fig:design}A). Instead of clicking and dragging into place, matchers simply clicked the one they believed was the target. They were not allowed to click until after a message is sent by the speaker.  We constructed a sequence of six blocks of twelve trials (for a total of 72 trials), where each tangram appeared once per block.%, and the same tangram was never the target twice in a row. 
Because targets were cued one at a time, numbers labeling each square in the grid were irrelevant and we removed them. The context of tangrams was scrambled on every trial, and participants were given full, immediate feedback: the director saw which tangram their partner clicked, and the matcher saw the intended tangram.

\subsection{Data pre-processing}

We used a three step pre-processing pipeline to prepare our corpus for subsequent analyses. Unless otherwise noted, we used the open-source Python package \texttt{spaCy} to implement all NLP tasks. 

\begin{enumerate}

\item \textbf{Spell-checking and regularization}: We conservatively extracted all tokens that did not exist in the vocabulary of the smallest available ($\sim$ 50,000 word) \texttt{spaCy} model and passed them through the SymSpell spell-checker \footnote{\texttt{https://github.com/wolfgarbe/SymSpell}}. These suggested corrections were then sequentially presented to the first author and either accepted or overridden at their judgement. This process constructed a reproducible spell-correction dictionary we applied to our dataset.

\item \textbf{Cleaning unrelated discourse}: Because we allowed our participants to interact in real-time through the chat box, many pairs produced text unrelated to the task of referring to the current target (e.g. greeting one another, asking personal questions, commenting on the length of the task or the results of previous rounds). We wanted to ensure that our structural results were not confounded by patterns in this kind of discourse across the task, and that the semantic content we observe on a particular trial is in fact being used to refer to the current target rather than task-irrelevant topics or, as we found in some cases, referring to other tangrams while debriefing previous errors. We therefore applied a manual pass applying a rubric that any text not directly referring to the current target is removed. For example, utterances like ``this is the one we got wrong last time'' were kept in because they were referring to a property of the current tangram, but utterances like ``good job'' and ``they'll go quicker if you remember what I say!'' are not. This process also created a reproducible JSON.

\item \textbf{Collapsing multiple messages within a round}: Finally, some speakers used our chat box like an texting interface, hitting the enter key between every micro-phrase of text. This made it difficult to interpret the output of syntactic parses. We therefore collapsed repeated messages by a participant within a round into a single message by inserting commas between successive messages. We chose to use commas because it tends to maintain grammaticality and does not inflate word counts.

\end{enumerate}

%Still, we verified that all results reported were robust to these exclusion criteria, and to our data pre-processing steps\todo{check}.

\section{How do meanings change over time?}

An inferential account makes three key predictions about how speakers change the content of their referring expressions over time.
\emph{First}, due to sources of variability in the population of speakers, we predict that the referring expressions used by different pairs will increasingly diverge to different, idiosyncratic labels.
In other words, different pairs will find different but equally successful equilibria in the space of possible linguistic conventions.
\emph{Second}, as speakers learn and gradually strengthen their expectations about how their partner will interpret their referring expressions, the labels used within each pair for each tangram with stabilize.
In other words, once there is evidence that a particular label is successfully understood, there is little reason to deviate from it.
\emph{Third}, if participants are also influenced by pragmatic pressures to be informative, the labels that conventionalize should not be a random draw from the initial description. 
Instead, we predict that more \emph{distinctive} words in the initial label (e.g. words used exclusively to describe one tangram) will be more likely to remain in later descriptions while less distinctive words contained in many initial descriptions are more likely to be dropped.

\subsection{Conventions diverge across pairs but converge to stable equilibria within pairs}

We examine the first two predictions using two different signatures of similarity: one based on distances computed between continuous vector embeddings of referring expressions and the other based on properties of the discrete word count distribution.

\subsubsection{Semantic embeddings}

\begin{figure}[t]
\centering
\includegraphics[scale=.3]{tsne_embeddings.pdf}
\caption{Semantic embeddings of referring expressions for each tangram, as they change over the game. \todo[inline]{Without really zooming in, it's harder to see the open/closed circles indicating beginning/end (color dominates)}}
\label{fig:tsne}
\end{figure}

Although the idea of using vector space representations of words to measure similarity is an old one \cite{XXX}, recent breakthroughs in machine learning have yielded rapid improvements in these representations \cite{XXX}.
To examine the dynamics of semantic context in referring expressions across and within games, we extracted the 300-dimensional GloVe vector for each word \cite{} and averaged word vectors to obtain a single vector for each referring expression.
To avoid artifacts from function words, we only included content words (nouns, adjectives, verbs) in this average.

In Fig. \ref{fig:tsne}, we visualize the trajectories of each game in a common vector space using dimensional reduction techniques.
Specifically, we feed the first 50 components recovered by PCA into the t-SNE algorithm, which stochastically embeds the semantic representations of each utterance used to refer to a given tangram in a common 2D vector space. 
Utterances from the same speaker are connected by lines, with the initial state marked by an open circle and final state marked by a closed circle.

We make two exploratory observations from this visualization.
First, while the initial utterances of games cluster tightly near the center of the space, the final utterances are \emph{dispersed} widely around the edges in a ring. 
This suggests that different speakers overlap more in their early descriptions before diverging to more idiosyncratic utterances late in the game.
Second, 

To test these observations more rigorously, we computed the average vector similarity of utterances used to refer to each tangram on different repetitions \emph{within} a game and also the average similarity \emph{across games}. 
We predicted that the similarity within games would be higher than the similarity across games, so the statistic of interest was the ratio of these two similarities.
We therefore bootstrapped this statistic, resampling different pairs and different tangrams with replacement, to obtain a confidence interval. 
We found that this ratio was significantly lower than unity, $CI: [], p = XXX$, supporting the observation that different pairs adopt different conventions while a single pair tends to keep using a convention once established.

%Idiosyncracy across pairs can additionally be assessed by testing the generalization performance of a classifier on held out pairs at the beginning and end of the game. Training on earlier \dots

\subsubsection{Using discrete word distributions}
Another way to quantify convergence and divergence across different pairs is to examine the \emph{distribution of words} that each pair uses to refer to each tangram.
If a pair of participants converges on stable labels for a tangram, then this stability should manifest in a highly structured distribution over words throughout the game for that pair.
If different speakers discover diverging conventions, this idiosyncracy should also manifest in differing word distributions.
We formalize these intuitions by examining the information-theoretic measure of entropy: $$H(W) = \sum_w P(w) \log P(w)$$
The entropy of the word distribution for a pair is maximized when all words are used equally often and declines as the distribution becomes more structured, i.e.~when the probability mass is more concentrated on a subset of words.

To compare word distributions across games, we use a permutation test methodology.
By scrambling utterances across games and recomputing the entropy of the scrambled word distribution, we effectively disrupt any distinctive structure within each pair.
There are two important inferences we can draw from this test.
First, in a null scenario where different pairs did \emph{not} diverge as predicted and instead every pair coordinated on roughly the same (optimal) convention for each tangram, this permutation operation would have no effect since it would be mixing together 
Second, in another null scenario where pairs did not converge and instead varied wildly in the words they used on each round, permuting across games would also have no effect since it would simply mix together word distributions that already have high entropy.
Hence, scrambling should \emph{increase} the average game's entropy only in the case where both predictions hold: each game's idiosyncratic, concentrated distribution of words would be mixed together to form more heterogeneous and therefore high-entropy distributions.

Following this logic, we computed the average within-game entropy for 1000 different permutations of speaker utterances. 
We permuted utterances within rounds rather than across the entire data set to control for the fact that earlier rounds may generically differ from later rounds. 
Since this permutation scheme keeps the number of messages per participant constant and simply swaps out the content of those messages, it also controls for the fact that some speakers sent more messages than others. 
We found that our null distribution lay within the interval $X, Y$, which is significantly higher than the true entropy (averaged across games) of $Z, p < 0.001$.


\subsection{More distinctive words on the first round are more likely to conventionalize}

Which words are dropped and which remain?


\section{Quantifying the dynamics of structure}\label{results}

\subsection{Dialogue between speaker and listener}\label{listener-feedback}

Before focusing our analysis on the rich dynamics of how \emph{directors} produce referring expressions over time, we first examine the bi-directional dynamics of dialogue exchanges. 
An influential result from is that conceptual pacts for referring are established \emph{collaboratively} \citet[see also \citealp{KraussWeinheimer66_Tangrams, GarrodFayLeeOberlanderMacLeod07_GraphicalSymbolSystems}]{clarkReferringCollaborativeProcess1986}: 
directors and matchers engage in a bi-directional process where matchers ask follow-up questions, suggest corrections, and acknowledge or verbally confirm their understanding through a backchannel. 
This theory predicts that (1) listener feedback should be highest on the first round and drop off once meanings are agreed upon, and (2) dyads with more initial listener feedback should reduce to more efficient conventions. 
Our use of a chat box rather than in-person verbal communication, along with the automatic feedback we provided each round in the \emph{cued} condition, may have diminished the bi-directionality somewhat, but we find correlational evidence of both patterns in our uncollapsed but otherwise cleaned data. 
The number of listener messages decreases significantly over the game $(b=-0.5, t = -10.6, p < 0.001)$, and there is a small but significant effect of the (logged) number of initial listener messages on overall \% reduction in the number of words used by the director, $r = 0.26, 95\% CI = [0.57, 0.45], p = 0.014$.
\todo[inline]{Note: this last result isn't actually saying much because when listeners say more on the first round, speaker often say more in response\dots they therefore have a higher initial word count to reduce from than if the listener didn't say anything, so it's not surprising that \% reduction is larger... we need to revise the analysis to control for this, e.g. by only considering reduction from the words used \emph{before} the listener's first message, i.e. what they \emph{would} have initially said if the listener didn't chime in? The more simple \& obvious regression formulation would be whether \# of initial listener messages predicts \emph{absolute} round 6 message length rather than \% reduction; this didn't come out, but I switched to the \% reduction version because I was worried that it was just due to pair-level variance in final round length}
\todo[inline]{This could also be a good place to put an effect of, like, more words used for tangrams that were incorrect in the previous round.}

\begin{figure}[t]
\includegraphics[scale=.65]{reduction.pdf}
\caption{(A) similar reduction in \# words per tangram for both variants of the task (B) word counts broken down by part of speech, combined across both variants (C) phrasal reduction based on syntactic parse \todo[inline]{Maybe showing \% reduction is better than raw numbers, i.e. normalizing by occurrence on first round (or version mike suggested w/ bars showing proportions at beginning and end.}}
\label{fig:reduction}
\end{figure}

\subsection{Reduction in number of words}\label{reduction}

Next, we turn to a set of analyses examining reduction in utterance length over the course of the experiment. 
At the coarsest level, we find that the mean number of words used by speakers decreases over time (see Fig. \ref{fig:replication}). 
This decrease replicates a highly reliable reduction effect found throughout the literature on iterated reference games (Brennan \& Clark, 1996; Krauss \& Weinheimer, 1964), although perhaps due to our purely textual (vs.~spoken) interface, participants in our task used many fewer words overall than previously reported. 
The following analyses break down this broad reduction into a finer-grained set of phenomena.

\subsection{Reduction in parts of speech}
The next level of granularity motivating our model approach concerns which kinds of words are most likely to be dropped. 
Is the speaker adopting a shorthand where they drop uninformative function words, or are they simplifying or narrowing their descriptions by omitting meaningful details (Clark \& Wilkes-Gibbs, 1986)? 
We used the Stanford CoreNLP part-of-speech tagger (Toutanova, Klein, Manning, \& Singer, 2003) to count the number of words belonging to each part of speech in each message. Fig. \ref{fig:pos} shows the percent reduction of different parts of speech from the first round to the sixth round. 
We find that determiners (`the', `a', `an') are the most likely class of words to be dropped with an X\% reduction rate, on average. 
Nouns (`dancer', `rabbit') are the least likely class to be dropped with only an Y\% rate. 
Closed-class parts of speech are strictly more likely to be dropped than open-class parts of speech.

While this finding suggests that speakers might just be adopting a
shorthand using more ungrammatical fragments as the game proceeds, we
find a more complex dynamic by examining the table of unigrams and
bigrams most likely to be dropped (see Table \ref{tab:words}). Note that
alongside dropped articles, there are a number of words that form
conjunctions (`and') and modifiers (`of', `with', `the right'). In other
words, it may be more likely that when function words are dropped, it is
primarily as part of larger grammatical units that provide additional
information in identifying the target.

\subsection{Reduction in syntactic constituents}
We explicitly examined this hypothesis by running the Stanford
constituency parser (Schuster \& Manning, 2016), tagging the occurrence
of subordinate/adverbial clauses (`sitting \emph{facing left}') and
adjectival clauses (`angel \emph{that is praying}').\footnote{Specifically,
  we used the Universal Dependencies tags \texttt{csubj, ccomp, xcomp},
  and \texttt{advcl} for subordinate clauses and \texttt{acl} for
  adjectival clauses (Schuster \& Manning, 2016)} We found that both
were reduced over the course of the game (see Fig.
\ref{fig:replication}), lending additional support for the hypothesis
that meaningful details are increasingly omitted. Initial phrases pile
on multiple ambiguous, partially redundant modifiers and descriptors: as
the game progresses and ambiguity of reference decreases, these
additional meaningful units become less useful and can be dropped.

This result accords with early observations by \cite{Carroll80_NamingHedges}, which found that in three-quarters of transcripts from \cite{KraussWeinheimer64_ReferencePhrases} the short names that participants converged upon were prominent in some syntactic construction at the beginning, often as a head noun that was initially modified or qualified by other information. 

\subsection{Shared dynamics across pairs}

Tree-edit distance\dots


\section{General Discussion}\label{general-discussion}

In this paper, we revisited the classic phenomenon of
convention-formation in a large-scale replication of the tangrams task,
finding evidence of arbitrariness and stability as well as finer-grained
reduction of meaningful clauses. 

%% Paragraph about opportunities for computational models
While we have focused on broader theoretical questions, our results also serve as a foundation for high-resolution task-performing computational models of communication seeking to explain the full richness of natural data. To build machines that naturally adapt to their interlocutors in human-robot or human-computer interaction scenarios, we must go behind qualitative efforts.

Theories of convention-formation vary in the extent to which social
reasoning about common ground is required. Our agents lie on a spectrum
between the heuristic updating agents of Barr (2004) and the
sophisticated agents of Clark \& Wilkes-Gibbs (1986), who
collaboratively build up explicit representations of mutual knowledge.
Speakers and listeners in our model implicitly coordinate their beliefs
through a shared history of observations, which serves as ``common
ground'' in an informal sense. They make critical use of pragmatic,
social reasoning in order to learn meanings, but do not explicitly
consider the fact that this history is shared, or represent their
partner's own uncertainty.

By capturing reduction, which purely heuristic theories have not yet
demonstrated, we showed that minimal assumptions of social reasoning go
a long way in accounting for key phenomena. Still, our model falls short
in some ways. For instance, because we do not provide a mechanisms for
the listener agent to respond with confirmation, repair, or follow-up
questions, we cannot make explicit predictions about the reduction in
\emph{listener messages} (as in Fig. \ref{fig:replication}) or the
impact of early listener responses on conventionalization. These
phenomena require our model to deal with planning over extended
dialogues, and to potentially weaken the assumption that one's partner
knows the true lexicon with complete certainty. Similarly, while our
model was explicitly designed with linguistic conventions in mind, it
remains to be seen whether the same formulation generalizes to broader
behavioral conventions. For example, the real-time coordination games
used in Hawkins \& Goldstone (2016) may not require players to reason
about a structured lexicon with noise, but an action policy
representation may play a similar role. While there remain many complex
aspects of convention-formation in communication games left for future
research, our approach nonetheless serves as a lower bound on the degree
of social reasoning needed to capture lexical conventions in these
games.

\section*{acknowledgements}
Acknowledgements should include contributions from anyone who does not meet the criteria for authorship (for example, to recognize contributions from people who provided technical help, collation of data, writing assistance, acquisition of funding, or a department chairperson who provided general support), as well as any funding or other support information.

\section*{conflict of interest}
You may be asked to provide a conflict of interest statement during the submission process. Please check the journal's author guidelines for details on what to include in this section. Please ensure you liaise with all co-authors to confirm agreement with the final statement.

\printendnotes

% Submissions are not required to reflect the precise reference formatting of the journal (use of italics, bold etc.), however it is important that all key elements of each reference are included.
\bibliography{library}

% \begin{biography}[example-image-1x1]{A.~One}
% Please check with the journal's author guidelines whether author biographies are required. They are usually only included for review-type articles, and typically require photos and brief biographies (up to 75 words) for each author.
% \bigskip
% \bigskip
% \end{biography}

% \graphicalabstract{example-image-1x1}{Please check the journal's author guildines for whether a graphical abstract, key points, new findings, or other items are required for display in the Table of Contents.}

\end{document}
