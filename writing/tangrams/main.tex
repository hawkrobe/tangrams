%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A template for Wiley article submissions.
% Developed by Overleaf. 
%
% Please note that whilst this template provides a 
% preview of the typeset manuscript for submission, it 
% will not necessarily be the final publication layout.
%
% Usage notes:
% The "blind" option will make anonymous all author, affiliation, correspondence and funding information.
% Use "num-refs" option for numerical citation and references style.
% Use "alpha-refs" option for author-year citation and references style.

\documentclass[alpha-refs]{wiley-article}
% \documentclass[blind,num-refs]{wiley-article}

% Add additional packages here if required
\usepackage{siunitx}
\usepackage{todonotes}
\graphicspath{{./figs/}}

% Update article type if known
\papertype{Original Article}
% Include section in journal if known, otherwise delete
%\paperfield{Journal Section}

\title{Dynamics of structure and content in repeated reference}

% List abbreviations here, if any. Please note that it is preferred that abbreviations be defined at the first instance they appear in the text, rather than creating an abbreviations list.
%\abbrevs{ABC, a black cat; DEF, doesn't ever fret; GHI, goes home immediately.}

% Include full author names and degrees, when required by the journal.
% Use the \authfn to add symbols for additional footnotes and present addresses, if any. Usually start with 1 for notes about author contributions; then continuing with 2 etc if any author has a different present address.
\author[1]{Robert Hawkins}
\author[1]{Michael Frank}
\author[1,2]{Noah Goodman}

%\contrib[\authfn{1}]{Equally contributing authors.}

% Include full affiliation details for all authors
\affil[1]{Department of Psychology, Stanford University}
\affil[2]{Department of Computer Science, Stanford University}

\corraddress{Robert Hawkins, Author One PhD, Department, Institution, City, State or Province, Postal Code, Country}
\corremail{rxdh@stanford.edu}

% \presentadd[\authfn{2}]{Department, Institution, City, State or Province, Postal Code, Country}

\fundinginfo{Funder One, Funder One Department, Grant/Award Number: 123456, 123457 and 123458; Funder Two, Funder Two Department, Grant/Award Number: 123459}

% Include the name of the author that should appear in the running header
\runningauthor{Hawkins et al.}

\begin{document}

\maketitle

% ABSTRACT THOUGHTS
% Framework for providing a quantitative characterization of phenomena; tracking meaningful content changing over time. how is the structure different? semantics and syntax... contents and structure.

% 1. syntax changes systematically in same-ish way for *everybody* (i.e. pruning, nominalization)
% 2. a lot of semantic idiosyncracy but internally consistent (i.e. meaning-preserving) within games (i.e. initialize in different places and become less redundant). cleaving utterances by preserving the relevant things to make the distinctions that matter for this task. stable on meaning but decreasing redundancy... 

% continuity in semantics & lose overlappingness... 
% compare semantic embeddings within game vs. across game?
% compare syntax within game vs. across game

% similarity on words that are there at the end vs. not there at the end...
% qualitative -- pull out unigrams and bigrams that are maintained per tangram
% Hinge from big picture to "this is a case study or task" surprisingly answerable by new tools

\begin{abstract}
How do speakers understand one another in conversation? 
When talking with novel partners about novel referents, speakers must continuously adapt their existing lexicon to coordinate on \emph{ad hoc} conventions for increasingly effective, efficient communication.
While such adaptation has been demonstrated under a variety of conditions, prior work has not focused on precisely \emph{how} the structure and content of natural language referring expressions change through continued interaction.
Here we draw upon recent advances in natural language processing to provide a quantitative characterization of these dynamics, taking as a case study a classic repeated reference game task using initially difficult-to-describe \emph{tangram} stimuli.
We collected a corpus ($>$15,000 utterances) of extended dyadic interactions in two variants of this task.
Structurally, we found that utterances reduce across pairs following similar trajectories: entire modifying syntactic units drop out as conventions are established, leaving only contentful open-class parts of speech.
When examining the semantics of referring expressions, however, we found that different pairs tend to diverge into a wide range of idiosyncratic conventions in a highly path-dependent manner:  
words that are more discriminitive in the initial context are more likely to persist through final conventions.
In sum, these findings highlight the systematicity and context-dependence of convention formation and lay the empirical foundation for higher-resolution theories of social adaptation.

% Please include a maximum of seven keywords
\keywords{social coordination, conventions, semantics, syntax, natural language processing, semantic embeddings}
\end{abstract}

\section{Introduction}\label{introduction}

%% Statement of the computational problem that motivates interest in this phenomenon?
Communication poses a challenging coordination problem for social agents. 
While a lifetime of learning the conventions of our language community provides a crucial backbone for understanding each other \citep{Lewis69_Convention}, we frequently find ourselves in situations where our existing lexicon falls short.
For one, no two speakers share exactly the same lexicon \cite{Davidson86_DerangementOfEpitaphs, Clark98_CommunalLexicons}. 
Lingo, proper nouns, nicknames, slang, inside jokes, metaphors, and other creative uses of neologism all operate in a regime of uncertainty where the same word or phrase may \emph{a priori} mean something different (or nothing at all) to different partners in different contexts. (cite, cite).
At the same time, because we live in a changing environment, we constantly experience novel entities, events, thoughts, and feelings we want to talk about---complex referents for which we have no pre-existing conventions and real uncertainty about whether a novel compositional expression will mean to our partner exactly what we intend it to mean.
What do we do when our lexicon isn't sufficient --- when we have to talk about something we've never had to talk about before with a partner we've never met?

A family of interactive communication experiments called \emph{repeated reference games} have provided a natural and productive paradigm for studying language use in such scenarios. In a seminal study by \cite{KraussWeinheimer64_ReferencePhrases}, pairs of participants played a cooperative language game where they were presented with arrays of ambiguous shapes in randomized orders. 
The players were allowed to talk freely; their goal was to find the correspondence between their boards by producing mutually understood descriptions of the shapes. 
Critically, as particular shapes reappeared on successive rounds of the game, descriptions were dramatically shortened: a lengthy early description like ``the upside-down martini glass in a wire stand'' became simply ``martini'' by the end. 
In other words, these results suggested that speakers addressed the computational challenge of referring to novel objects with a novel partner by initially drawing on many sources of prior information from existing conventions and then \emph{adapting} with their partner based on shared history.

This signature reduction effect has been replicated under many conditions testing the boundaries of adaptation, manipulating the kinds of objects used as targets, the contexts in which the objects appear, the identity of one's partner across repetitions, the feedback available, and the medium participants use to communicate. (cite, cite, cite). 
While this canon of qualitative effects has played an influential role in shaping theories of social coordination in communication, particularly in debates over the role of common ground representations, a \emph{quantitative} characterization of precisely what gets reduced, and how, has remained elusive. 
Yet these details matter for a number of open questions. 
How systematic is the structure of changes over time?
To what extent are idiosyncratic\dots
\todo[inline]{Refine these questions}

Here we present a large corpus of utterances from a web-based 

\emph{Arbitrariness} is a definitional property of conventions (Lewis,
1969): there must be multiple solutions that would be equally successful
as long as both players ``agree'' (e.g.~driving on the left vs.~right
side of the road). By the final round in a language game, for example,
the same tangram might be called the `dancer' to one pair and the
`skater' to another. The other definitional property we consider is
\emph{stability}: it is in everyone's best interest to keep using the
convention, once established. Finally, \emph{reduction} is more specific
to the reference game paradigm and refers to the transformation of
longer, complex expressions into simpler expressions over the course of
interaction, as Krauss \& Weinheimer (1964) observed. While this broad
phenomenon has been replicated many times, exactly what is reduced
remains an open empirical question.

Theories of convention-formation differ primarily in the extent to which
sophisticated social reasoning and common ground is required. At one
extreme, agents use simple heuristic updating rules and do not need to
represent or reason about other agents at all (Barr, 2004; Centola \&
Baronchelli, 2015; Young, 2015). Simulations elegantly show how
arbitrary signaling systems can spread and come to dominate large
populations. However, due to their `rich get richer' dynamic, it is not
clear how emergence-through-use mechanisms alone could account for
reduction in repeated interaction. At the other extreme, are theories in
which agents explicitly consider their partner's beliefs and track what
information is \emph{mutual knowledge}, often formalized in a game
theoretic setting (Lewis, 1969). Wilkes-Gibbs \& Clark (1992) and others
have proposed that agents engage in a collaborative process of
establishing mutual knowledge, though the mechanisms allowing
conventions to emerge under such conditions have not been instantiated
in a formal model to our knowledge.

%In this paper, we argue for a theoretical position on the spectrum
%between these poles: \emph{conventions form when agents assume
%conventions already exist}. In other words, agents believe there is a
%true lexicon used by other agents but are initially unsure of its
%identity. Through their interactions with a partner who is assumed to be
%knowledgeable and informative, agents can learn this true lexicon even
%though their partner in fact begins in the same state of ignorance.
%Agents thus coordinate on the same lexicon, which becomes conventional.

To support this theory, we first conducted a large-scale, multi-player
replication of the tangrams task, which has traditionally been limited
to relatively small sample sizes in the lab. We then demonstrate signatures
of arbitrariness, stability, and reduction which have been difficult to
study at a fine-grained level due to the sparseness of existing data.
%Next, we formulate our theory in a computational model of communication
%in repeated reference games, based on recent successes capturing
%language understanding as social inference (Goodman \& Frank, 2016;
%Goodman \& Stuhlm√ºller, 2013) and show that this model qualitatively
%produces all three empirical signatures.

\section{Methods}

To collect a corpus of natural dialogue that supports quantitative analyses of convention formation over time, we ported the tangrams task used in \cite{ClarkWilkesGibbs86_ReferringCollaborative} to a real-time, multi-player web environment. 
We developed two variants of the game: a \emph{free-matching} version that closely replicates classic in-lab designs and a more tightly controlled \emph{cued} version that allows for higher resolution analyses of how references to individual tangrams changed over time (see Fig. \ref{fig:design}). 

\begin{figure}
\includegraphics[scale=.65]{designAndExample.pdf}
\caption{(A) Screenshots of the `free matching' and `cued' variants of the tangrams task; (B) Visualization of words used to refer to one tangram on the first round compared to the final round. \todo[inline]{Fix text/formatting in screenshots (maybe use scheme instead); think about less corny representation than wordcloud? figure out where to put the wavy guy, maybe add speech bubble to indicate words came from speaker?}}
\label{fig:design}
\end{figure}

\subsection{Exp.~1a: Free matching}

\subsubsection{Participants}\label{participants}

200 participants were recruited from Amazon's Mechanical Turk and paired
into dyads to play a real-time communication game using the framework in
Hawkins (2015). We excluded games that terminated before the completion
of 6 rounds and where participants reported a native language different
from English, leaving a corpus of X complete games with a total of
X utterances.

\subsubsection{Stimuli}\label{stimuli}

On every trial of the game, both participants were shown a \(6 \times 2\) grid containing twelve tangram shapes, reproduced from \cite{ClarkWilkesGibbs86_ReferringCollaborative}. 
Cells were labeled with fixed numbers from one to twelve in order to help participants easily refer to locations in the grid (see Fig. \ref{fig:design}).

\subsubsection{Procedure}\label{procedure}

After passing a short quiz about task instructions, participants were
randomly assigned the role of either `director' or `matcher' and
automatically paired into virtual rooms containing a chat box and grid
of stimuli. Both participants could freely use the chat box to
communicate at any time. The director's tangrams were fixed in place,
but the matcher could click and drag the shapes to reorder them. The
director had to send messages about the locations of different tangrams
on their fixed board (e.g. ``\#1 looks like an X'', ``2 is the one with
the Y''); the matcher had to identify the corresponding tangram shapes
and move them to the correct locations. When the players were satisfied
that their boards matched, the matcher clicked a `submit' button that
gave players feedback on their score (out of 12) and scrambled the
tangrams for the next round. After six rounds, players were redirected
to a short exit survey. We collected the raw text of every message sent
and every swapping action taken by the matcher.

\section{Exp.~1b: Cued reference}

While the free matching design we replicated in Exp.~1a is highly naturalistic, it also provides messy data. Utterances included not only descriptions of the tangrams but also information about the desired location (e.g. 'number 10 is the \dots'). Additionally, because participants often revisited tangrams out of order and mentioned multiple tangrams in a single message, it was difficult to isolate exactly which utterances referred to which tangrams without extensive hand-annotation. Additionally, players occasionally advanced to the next round without referring to all 12 tangrams. In order to conduct analyses at the tangram-by-tangram level, we designed a straightforwardly sequential variation on the task where speakers are privately cued to refer to targets one-by-one and feedback is given on each round. 

\subsection{Participants}\label{participants}

XYZ participants were recruited from Amazon's Mechanical Turk and paired
into dyads. We excluded games that terminated before the completion
of the experiment, and where participants reported a native language different
from English, leaving a corpus of X complete games with a total of
X utterances.

\subsection{Stimuli \& Procedure}\label{stimuli}

On each trial, one of the twelve tangrams was privately highlighted with a box as the \emph{target} for the director (see Fig. \ref{fig:design}A). The director's goal was to produce a referring expression such that their partner could accurately select the target from their own context. Instead of clicking and dragging into place, as in Exp.~1a, listeners simply clicked the one they believed was the target. We constructed a sequence of six blocks of twelve trials (for a total of 72 trials), where each tangram appeared once per block, and the same tangram was never the target twice in a row. We used the same stimuli as Exp.~1b, but because targets were cued one at a time, numbers labeling each square in the grid were irrelevant and we removed them. The context of tangrams was scrambled on every trial, and participants were given full, immediate feedback: the director saw which tangram their partner clicked, and the matcher saw the intended tangram.

\section{Changes in structure}\label{results}

\begin{figure}[t]
\includegraphics[scale=.65]{reduction.pdf}
\caption{(A) similar reduction in \# words per tangram for both variants of the task (B) word counts broken down by part of speech, combined across both variants (C) phrasal reduction based on syntactic parse \todo[inline]{Maybe showing \% reduction is better than raw numbers, i.e. normalizing by occurrence on first round (or version mike suggested w/ bars showing proportions at beginning and end.}}
\label{fig:reduction}
\end{figure}

\subsection{Reduction}\label{reduction}

Next, we turn to a set of analyses examining reduction in utterance
length over the course of the experiment. At the coarsest level, we find
that the mean number of words used by speakers decreases over time (see
Fig. \ref{fig:replication}). This decrease replicates a highly reliable
reduction effect found throughout the literature on iterated reference
games (Brennan \& Clark, 1996; Krauss \& Weinheimer, 1964), although
perhaps due to our purely textual (vs.~spoken) interface, participants
in our task used many fewer words overall than previously reported. The
following analyses break down this broad reduction into a finer-grained
set of phenomena.

The next level of granularity motivating our model approach concerns
which kinds of words are most likely to be dropped. Is the speaker
adopting a shorthand where they drop uninformative function words, or
are they simplifying or narrowing their descriptions by omitting
meaningful details (Clark \& Wilkes-Gibbs, 1986)? We used the Stanford
CoreNLP part-of-speech tagger (Toutanova, Klein, Manning, \& Singer,
2003) to count the number of words belonging to each part of speech in
each message. Fig. \ref{fig:pos} shows the percent reduction of
different parts of speech from the first round to the sixth round. We
find that determiners (`the', `a', `an') are the most likely class of
words to be dropped with an X\% reduction rate, on average. Nouns
(`dancer', `rabbit') are the least likely class to be dropped with only
an Y\% rate. Closed-class parts of speech are strictly more likely to be
dropped than open-class parts of speech.

While this finding suggests that speakers might just be adopting a
shorthand using more ungrammatical fragments as the game proceeds, we
find a more complex dynamic by examining the table of unigrams and
bigrams most likely to be dropped (see Table \ref{tab:words}). Note that
alongside dropped articles, there are a number of words that form
conjunctions (`and') and modifiers (`of', `with', `the right'). In other
words, it may be more likely that when function words are dropped, it is
primarily as part of larger grammatical units that provide additional
information in identifying the target.

We explicitly examined this hypothesis by running the Stanford
constituency parser (Schuster \& Manning, 2016), tagging the occurrence
of subordinate/adverbial clauses (`sitting \emph{facing left}') and
adjectival clauses (`angel \emph{that is praying}').\footnote{Specifically,
  we used the Universal Dependencies tags \texttt{csubj, ccomp, xcomp},
  and \texttt{advcl} for subordinate clauses and \texttt{acl} for
  adjectival clauses (Schuster \& Manning, 2016)} We found that both
were reduced over the course of the game (see Fig.
\ref{fig:replication}), lending additional support for the hypothesis
that meaningful details are increasingly omitted. Initial phrases pile
on multiple ambiguous, partially redundant modifiers and descriptors: as
the game progresses and ambiguity of reference decreases, these
additional meaningful units become less useful and can be dropped.

This result accords with early observations by \cite{Carroll80_NamingHedges}, which found that in three-quarters of transcripts from \cite{KraussWeinheimer64_ReferencePhrases} the short names that participants converged upon were prominent in some syntactic construction at the beginning, often as a head noun that was initially modified or qualified by other information. 

\subsubsection{Listener feedback}\label{listener-feedback}

Finally, the theory proposed by Clark \& Wilkes-Gibbs (1986) argues that
lexical conventions are established through a collaborative process
requiring both speaker and listener input. This predicts that (1)
listener feedback should be highest on the first round and drop off once
meanings are agreed upon, and (2) dyads with more initial listener
feedback should converge on more efficient conventions. We find
correlational evidence of both patterns in our data. The number of
listener messages decreases significantly over the game (\(t = -13.23\),
see Fig. \ref{fig:replication}), and there is a weak but significant
effect of initial listener messages on overall reduction (X).

\begin{figure}[t]
\centering
\includegraphics[scale=.3]{tsne_embeddings.pdf}
\caption{Semantic embeddings of referring expressions for each tangram, as they change over the game. \todo[inline]{Without really zooming in, it's harder to see the open/closed circles indicating beginning/end (color dominates)}}
\label{fig:reduction}
\end{figure}

\section{Changes in semantics}

\subsection{Permutation tests}\label{arbitrariness-and-stability}

We begin by examining signatures of \emph{arbitrariness} and
\emph{stability} in our data. We operationalize these concepts using the
information-theoretic measure of entropy:
\[H(W) = \sum_w P(w) \log P(w)\] Broadly speaking, entropy measures the
predictability of a distribution. It is maximized when all elements are
equally likely and declines as the distribution becomes more structured,
i.e.~when the probability mass is concentrated on a subset of elements.

To derive predictions, we consider a permutation-test null model in
which utterances are scrambled within each round. The empirical entropy
of individual games should only differ from the null distribution if
\emph{both} arbitrariness and stability hold. First, note that if
stability did \emph{not} hold, scrambling would have no effect on the
entropy within individual games: speakers would already use different
words each round, and swapping out the identity of those words would not
affect the entropy of the word distribution.

If stability holds but arbitrariness does not, all players would adopt
the single optimal (non-arbitrary) way to refer to each tangram.
Therefore, the entropy of their word distributions also should not be
affected by scrambling: a speaker's real words would be swapped out for
the same words, just generated by another speaker. Finally, if both
arbitrariness and stability hold, then different speakers adopt
different referring expressions that persist from round to round. Hence,
scrambling should \emph{increase} the average game's entropy from a
relatively low level: each game's idiosyncratic, concentrated
distribution of words would be mixed together to form more heterogeneous
and therefore high-entropy distributions.

To test this prediction, we computed the average within-game entropy for
1000 different permutations of speaker utterances. We permuted
utterances within rounds rather than across the entire data set to
control for the fact that earlier rounds have longer utterances and thus
a larger vocabulary than later rounds (see the following section). Since
this permutation scheme keeps the number of messages per participant
constant and simply swaps out the content of those messages, it also
controls for the fact that some speakers sent more messages than others.
We found that our null distribution lay within the interval {[}X, Y{]},
which is significantly higher than the true entropy (averaged across
games) of Z \(p < 0.001\). This pattern is consistent only with
signatures of both arbitrariness and stability.

\subsection{Semantic embeddings}\label{arbitrariness-and-stability}

While words can be treated as discrete tokens, as in the previous section, 


\section{General Discussion}\label{general-discussion}

In this paper, we revisited the classic phenomenon of
convention-formation in a large-scale replication of the tangrams task,
finding evidence of arbitrariness and stability as well as finer-grained
reduction of meaningful clauses. 

%% Paragraph about opportunities for computational models
While we have focused on broader theoretical questions, our results also serve as a foundation for high-resolution task-performing computational models of communication seeking to explain the full richness of natural data. To build machines that naturally adapt to their interlocutors in human-robot or human-computer interaction scenarios, we must go behind quaitative effecrts.

Theories of convention-formation vary in the extent to which social
reasoning about common ground is required. Our agents lie on a spectrum
between the heuristic updating agents of Barr (2004) and the
sophisticated agents of Clark \& Wilkes-Gibbs (1986), who
collaboratively build up explicit representations of mutual knowledge.
Speakers and listeners in our model implicitly coordinate their beliefs
through a shared history of observations, which serves as ``common
ground'' in an informal sense. They make critical use of pragmatic,
social reasoning in order to learn meanings, but do not explicitly
consider the fact that this history is shared, or represent their
partner's own uncertainty.

By capturing reduction, which purely heuristic theories have not yet
demonstrated, we showed that minimal assumptions of social reasoning go
a long way in accounting for key phenomena. Still, our model falls short
in some ways. For instance, because we do not provide a mechanisms for
the listener agent to respond with confirmation, repair, or follow-up
questions, we cannot make explicit predictions about the reduction in
\emph{listener messages} (as in Fig. \ref{fig:replication}) or the
impact of early listener responses on conventionalization. These
phenomena require our model to deal with planning over extended
dialogues, and to potentially weaken the assumption that one's partner
knows the true lexicon with complete certainty. Similarly, while our
model was explicitly designed with linguistic conventions in mind, it
remains to be seen whether the same formulation generalizes to broader
behavioral conventions. For example, the real-time coordination games
used in Hawkins \& Goldstone (2016) may not require players to reason
about a structured lexicon with noise, but an action policy
representation may play a similar role. While there remain many complex
aspects of convention-formation in communication games left for future
research, our approach nonetheless serves as a lower bound on the degree
of social reasoning needed to capture lexical conventions in these
games.

\section*{acknowledgements}
Acknowledgements should include contributions from anyone who does not meet the criteria for authorship (for example, to recognize contributions from people who provided technical help, collation of data, writing assistance, acquisition of funding, or a department chairperson who provided general support), as well as any funding or other support information.

\section*{conflict of interest}
You may be asked to provide a conflict of interest statement during the submission process. Please check the journal's author guidelines for details on what to include in this section. Please ensure you liaise with all co-authors to confirm agreement with the final statement.

\printendnotes

% Submissions are not required to reflect the precise reference formatting of the journal (use of italics, bold etc.), however it is important that all key elements of each reference are included.
\bibliography{library}

% \begin{biography}[example-image-1x1]{A.~One}
% Please check with the journal's author guidelines whether author biographies are required. They are usually only included for review-type articles, and typically require photos and brief biographies (up to 75 words) for each author.
% \bigskip
% \bigskip
% \end{biography}

% \graphicalabstract{example-image-1x1}{Please check the journal's author guildines for whether a graphical abstract, key points, new findings, or other items are required for display in the Table of Contents.}

\end{document}
